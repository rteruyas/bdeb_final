name: Register Best Model

on:
  # Run manually from the Actions tab
  workflow_dispatch:
  
  # Run on schedule (once a day at midnight UTC)
  # schedule:
  #   - cron: '0 0 * * *'

jobs:
  find-and-register-best-model:
    runs-on: ubuntu-latest
    
    steps:
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install mlflow pandas
        
    - name: Create model registration script
      run: |
        cat > register_best_model.py << 'EOL'
        import os
        import mlflow
        from mlflow.tracking import MlflowClient

        def register_best_model():
            """
            Search for all runs ending with 'final_evaluation' across all experiments,
            find the best performing model based on accuracy, and register it to the MLflow Model Registry.
            """
            # Set MLflow tracking URI from environment variable
            tracking_uri = os.environ.get("MLFLOW_TRACKING_URI")
            if tracking_uri:
                mlflow.set_tracking_uri(tracking_uri)
                print(f"Using MLflow tracking URI: {tracking_uri}")
            else:
                print("MLFLOW_TRACKING_URI not set. Using default tracking URI.")
            
            # Set any required authentication
            if "MLFLOW_TRACKING_USERNAME" in os.environ and "MLFLOW_TRACKING_PASSWORD" in os.environ:
                os.environ["MLFLOW_TRACKING_TOKEN"] = f"{os.environ['MLFLOW_TRACKING_USERNAME']}:{os.environ['MLFLOW_TRACKING_PASSWORD']}"
                print("Using authentication from environment variables")
            
            print("Searching for experiments...")
            # Get all experiments
            experiments = mlflow.search_experiments()
            print(f"Found {len(experiments)} experiments")
            
            # Track the best model
            best_metric_value = float('-inf')
            best_run_id = None
            best_model_name = None
            
            # The metric to compare models
            target_metric = "accuracy"
            
            print(f"Searching for final evaluation runs and comparing by {target_metric}...")
            
            for experiment in experiments:
                experiment_id = experiment.experiment_id
                experiment_name = experiment.name
                
                print(f"Checking experiment: {experiment_name}")
                
                # Search for runs with names ending in 'final_evaluation'
                try:
                    runs = mlflow.search_runs(
                        experiment_ids=[experiment_id],
                        filter_string="attributes.run_name LIKE '%final_evaluation'"
                    )
                    print(f"  Available columns: {list(runs.columns)}")
                except Exception as e:
                    print(f"  Error searching runs: {e}")
                    continue
                
                if len(runs) == 0:
                    print(f"  No final evaluation runs found in {experiment_name}")
                    continue
                    
                print(f"  Found {len(runs)} final evaluation runs")
                
                # Process each run to find the best model
                for _, run in runs.iterrows():
                    run_id = run['run_id']
                    run_name = run['tags.mlflow.runName'] if 'tags.mlflow.runName' in run else run_id
                    
                    # Check if the target metric exists in this run
                    metric_column = f"metrics.{target_metric}"
                    if metric_column in run:
                        metric_value = run[metric_column]
                        print(f"  Run: {run_name}, {target_metric}: {metric_value}")
                        
                        # Compare with the best metric value (assuming higher is better)
                        if metric_value > best_metric_value:
                            best_metric_value = metric_value
                            best_run_id = run_id
                            best_model_name = run_name
                            print(f"  New best model found: {run_name} with {target_metric}: {metric_value}")
            
            # Register the best model
            if best_run_id:
                print(f"\nBest model found:")
                print(f"  Model: {best_model_name}")
                print(f"  {target_metric}: {best_metric_value}")
                print(f"  Run ID: {best_run_id}")
                
                # Register the model directly from the run
                try:
                    print(f"\nRegistering model to MLflow Model Registry...")
                    client = MlflowClient()
                    
                    # Define a model name
                    model_name = os.environ.get("MODEL_NAME", "best_model")
                    
                    # Use the standard model path
                    model_uri = f"runs:/{best_run_id}/model"
                    print(f"Using model URI: {model_uri}")
                    
                    # Check if model exists in registry
                    try:
                        existing_models = client.search_registered_models(filter_string=f"name='{model_name}'")
                        model_exists = len(existing_models) > 0
                        if not model_exists:
                            client.create_registered_model(model_name)
                            print(f"Created new registered model: {model_name}")
                    except Exception as e:
                        print(f"Note: Error checking if model exists (will try to create): {e}")
                        try:
                            client.create_registered_model(model_name)
                            print(f"Created new registered model: {model_name}")
                        except Exception as e2:
                            print(f"Note: Error creating model (it might already exist): {e2}")
                    
                    # Register the model
                    print(f"Registering model from run {best_run_id}...")
                    model_version = mlflow.register_model(
                        model_uri=model_uri,
                        name=model_name
                    )
                    version = model_version.version
                    print(f"Successfully registered model version: {version}")
                    
                    # Transition to Production
                    print(f"Setting model to Production stage...")
                    client.transition_model_version_stage(
                        name=model_name,
                        version=version,
                        stage="Production"
                    )
                    print(f"Model {model_name} version {version} is now in Production stage")
                    
                    # Set description
                    client.update_model_version(
                        name=model_name,
                        version=version,
                        description=f"Best model with {target_metric}={best_metric_value:.4f} from run: {best_model_name}"
                    )
                    
                    print(f"\nSUCCESS: Registered model as Production in the MLflow Model Registry")
                    print(f"Model name: {model_name}")
                    print(f"Model version: {version}")
                    print(f"Model {target_metric}: {best_metric_value:.4f}")
                    print(f"Original run: {best_model_name}")
                
                except Exception as e:
                    print(f"Error registering model: {str(e)}")
                    print(f"Run ID attempted: {best_run_id}")
                    print(f"Model URI attempted: {model_uri}")
                    raise
            else:
                print(f"\nNo models found with metric '{target_metric}' across all final evaluation runs.")

        if __name__ == "__main__":
            try:
                register_best_model()
            except Exception as e:
                print(f"Error: {e}")
                exit(1)
        EOL
      
    - name: Run model registration
      env:
        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
        MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_TRACKING_USERNAME }}
        MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_TRACKING_PASSWORD }}
        MODEL_NAME: "best_model"
      run: |
        python register_best_model.py